{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b36a1dcb-2e8e-4543-9be4-30011bd37847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': 'The last man on the earth was sitting in his room and suddenly there was a knock on the door.\\n\\n\"Do you know what happened?\" he asked.\\n\\n\"You got a call today from the police,\" said the man.\\n\\n\"What do you think?\" asked the man.\\n\\n\"It\\'s my mother\\'s house. She\\'s mad.\"\\n\\n\"I just saw her coming home.\"\\n\\n\"You mean the first time she got in the house, got in the car and got in the car?\"\\n\\n\"I don\\'t understand. I don\\'t know if she\\'s insane or not. I don\\'t know if she\\'s mad, but I don\\'t know if she\\'s crazy. I don\\'t know what she\\'s doing, if she\\'s crazy, she\\'s just out there. I don\\'t know if she\\'s mad, but I don\\'t know if she\\'s crazy.\"\\n\\n\"But she\\'s here and I don\\'t know if she\\'s crazy or not. I just don\\'t know if she\\'s mad, but I don\\'t know if she\\'s crazy.\"\\n\\n\"But she tried to help you, she tried to help you. But she tried to help you. But she was just out in the street and she was looking for you. She said, \\'I need help, I need help. I'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Creating a Tex-Generation Object\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# Example Prompt\n",
    "Prompt = \"The last man on the earth was sitting in his room and suddenly there was a knock on the door\"\n",
    "\n",
    "\n",
    "# Generating Output\n",
    "Output = generator(Prompt, max_length=100, num_return_sequences=3)\n",
    "\n",
    "print(Output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9685f56-9288-4764-b03c-5867564a102e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last man on the earth was sitting in his room and suddenly there was a knock on the door. When he opened the door, he saw the two men in dark shirts, grey collared shirts and dark pants. I think it was in an old church before I heard them. They were the people who tried to keep the church on guard from the church. I got up, took out the book I was carrying and started running back in the crowd.\"\n",
      "\n",
      "'A man has been shot,' says police reporter. 'I heard it before I met him... I got up. I was a bit scared'\n",
      "\n",
      "\"The shooting went over on the other side of the hall and all those men got up. Some of them ran towards me, then went back to see if there were any officers on this road. They did so, and I got up at 3 o'clock.\"\n",
      "\n",
      "\"One man said that we are the worst. The last man in town, was standing next to me and was talking about getting a gun. That guy shot and killed another man. We have now moved on. I can't see any of these shots because it was still dark,\" he says.\n",
      "\n",
      "The three men shot were one police sergeant, a sergeant-at-arms who works at the community policing department, and two police officers who patrol a busy traffic\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a Text-Generation pipeline using GPT-2\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# Example Prompt\n",
    "prompt = \"The last man on the earth was sitting in his room and suddenly there was a knock on the door\"\n",
    "\n",
    "# Generate output\n",
    "output = generator(\n",
    "    prompt,\n",
    "    max_length=100,       # The maximum number of tokens (words/pieces of words) in the final text\n",
    "                         # Includes both the prompt and generated continuation.\n",
    "                         # If too small → output cuts early. If too large → more text, but risk of rambling.\n",
    "\n",
    "    num_return_sequences=3,  # How many different continuations to return for the same prompt.\n",
    "                             # Example: 3 → you’ll get 3 unique stories.\n",
    "\n",
    "    temperature=1.2,         # Controls randomness/creativity:\n",
    "                             # - 0.7 = safe, focused, less random\n",
    "                             # - 1.0 = balanced (default)\n",
    "                             # - 1.2 = more creative, but risk of nonsense/repetition\n",
    "\n",
    "    top_k=50,                # At each step, only consider the top 50 most likely next words.\n",
    "                             # This keeps the model from choosing extremely rare/unlikely words.\n",
    "\n",
    "    top_p=0.9                # (a.k.a nucleus sampling) → choose from words that together make up 90% of probability mass.\n",
    "                             # Example: If top 5 words already cover 90%, it ignores the rest.\n",
    "                             # Helps keep text fluent while still varied.\n",
    ")\n",
    "\n",
    "# Print the first generated sequence\n",
    "print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3963f7f5-3789-498c-adbd-5799441a6e30",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ollama'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mollama\u001b[39;00m\n\u001b[0;32m      3\u001b[0m response \u001b[38;5;241m=\u001b[39m ollama\u001b[38;5;241m.\u001b[39mchat(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistral\u001b[39m\u001b[38;5;124m\"\u001b[39m, messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      4\u001b[0m   {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe last man on earth was sitting in his room and suddenly there was a knock on the door\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m      5\u001b[0m ])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ollama'"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(model=\"mistral\", messages=[\n",
    "  {\"role\": \"user\", \"content\": \"The last man on earth was sitting in his room and suddenly there was a knock on the door\"}\n",
    "])\n",
    "\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5a67f1-bfff-4526-893a-ff91bc3edb64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
